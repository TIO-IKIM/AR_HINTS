{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping raw eye/head-tracking data recorded with the HL2.\n",
    "**Data was recorded with the in house developed MR-OST-HMD application which simulates the HINTS exam.** <br>\n",
    "*Note: in this case the data is from the same healthy subject, we do not intend to share patient data at this stage.*\n",
    "\n",
    "### From raw recorded data to sorted data:\n",
    "The raw data (txt) files saved by the Unity application will be transformed to multiple CSV files.<br>\n",
    "*Per subject, one CSV file per part of the examination:* \n",
    "- (1) Freely looking around in the room.\n",
    "- (2) Looking at the floor. \n",
    "- (3) Looking at the ceiling. \n",
    "- (4) Looking at the nose of the examiner (at +- 40 cm).\n",
    "- (5) Looking Left of the examinar's nose (at +- 40 cm). \n",
    "- (6) Looking right of the examiner's nose (at +- 40 cm).\n",
    "- (7) Looking forward.\n",
    "- (8) During the HIT test while focussing at a black dot on the while at +- 40 cm.\n",
    "\n",
    "The CSV files are also grouped per class (symptoms diagnosed during conventional clinical exam). <br>\n",
    "*Please note that this depending on the folder structure.* <br>\n",
    "*Not adhering to our structure will result in all subjects stored in the 'Other' folder*\n",
    "\n",
    "Please also note that since this is for a public Github the data is taken from a single healthy subject. <br>\n",
    "In other words, even though we have seven classes, in this case it's the same healthy subject within each class. <br>\n",
    "The main goal is to show transparency within the data processing and improve reproducibility.\n",
    "\n",
    "### Processed data to features and labels\n",
    "The processed data will not be transformed to features and labels in the *'hints_features_labels'* folder.<br>\n",
    "In the *'hints_features_labels'* folder, there will be subfolders for each part of the exam, see above. <br>\n",
    "In each subfolder, you will find the *'features'* and *'labels'* folder with all the subjects ordered from 0, 1, ..., n. <br> \n",
    "\n",
    "**The labels indicate the following classes:**\n",
    "- 0 -> Healthy\n",
    "- 1 -> Skew\n",
    "- 2 -> Saccades\n",
    "- 3 -> NystagmusLeft\n",
    "- 4 -> NystagmusRight\n",
    "- 5 -> NystagmusDownbeating \n",
    "- 6 -> Other\n",
    "\n",
    "*Names for reference: ['Healthy', 'Skew', 'Saccades', 'NystagmusLeft', 'NystagmusRight', 'NystagmusDownbeating', 'Other']*<br>\n",
    "\n",
    "## order of cells\n",
    "1. Find the paths of the most up to data raw data (.txt) file per subject. <br><br>\n",
    "2. Create a main data folder (*'data/csv_per_patient_per_measurement_action'*) and subfolders per class with a subfolder per patient. <br>\n",
    "Then create a .csv file containing the relevant parts of the raw data for each part of the examination; <br><br>\n",
    "3. Create a features and labels folder (*'data/hints_features_labels'*) with subfolders for each part of the examination. <br>\n",
    "Inside the subfolders create a *'features'* and *'lables'* folder containing clean and sorted data of the eye-and-head-tracking measurements. <br><br>\n",
    "\n",
    "*Note that the application was made in Unity.*<br>\n",
    "*The extended eye tracking library for the HL2 is used for eye-tracking measurements per eye,*<br>\n",
    "*and the Transform information of the headset for the head measurements.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get the most recent .txt files from each patient/subject (i.e. per directory)\n",
    "The HL2 unity application can save multiple .txt files per patient/subject.\n",
    "Therefore we only keep the most recent one, which includes the most tracking data, and has the most recent timestamp.\n",
    "'''\n",
    "\n",
    "# Standard library imports\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# extract path of the most recent .txt files from each patient/subject directory\n",
    "def extract_latest_txt_files(\n",
    "    data_directory: str,\n",
    "    classes: List[str]\n",
    ") -> Tuple[Dict[str, List[str]], int]:\n",
    "    \"\"\"Extract paths of the most recent .txt files from each class directory.\n",
    "\n",
    "    Walks through the directory tree to find the most recent .txt file in each\n",
    "    subdirectory, then organizes them by class based on the directory structure.\n",
    "\n",
    "    Args:\n",
    "        data_directory: Root directory containing the raw data files\n",
    "        classes: List of class names to organize files into\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - Dictionary mapping class names to lists of file paths\n",
    "            - Total count of .txt files found\n",
    "    \"\"\"\n",
    "    txt_file_paths = []\n",
    "    total_txt_files = 0\n",
    "\n",
    "    # Walk through directory tree\n",
    "    for root, _, filenames in os.walk(data_directory):\n",
    "        newest_file = None\n",
    "        newest_time = None\n",
    "\n",
    "        # Find newest .txt file in current directory\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            total_txt_files += 1\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Parse timestamp from filename \n",
    "            # (format: 2024_01_24_09_32_48_longmessage.txt)\n",
    "            timestamp_match = re.search(\n",
    "                r'\\d{4}_\\d{2}_\\d{2}_\\d{2}_\\d{2}_\\d{2}',\n",
    "                filename\n",
    "            )\n",
    "            if not timestamp_match:\n",
    "                continue\n",
    "\n",
    "            timestamp = datetime.datetime.strptime(\n",
    "                timestamp_match.group(),\n",
    "                '%Y_%m_%d_%H_%M_%S'\n",
    "            )\n",
    "            if newest_time is None or timestamp > newest_time:\n",
    "                newest_file = file_path\n",
    "                newest_time = timestamp\n",
    "\n",
    "        if newest_file:\n",
    "            txt_file_paths.append(newest_file)\n",
    "\n",
    "    # Organize files by class\n",
    "    txt_paths = {folder_name: [] for folder_name in classes}\n",
    "    \n",
    "    for file_path in txt_file_paths:\n",
    "        # Extract class name from directory structure\n",
    "        folder_name = os.path.basename(\n",
    "            os.path.dirname(os.path.dirname(file_path))\n",
    "        )\n",
    "        if folder_name in classes:\n",
    "            txt_paths[folder_name].append(file_path)\n",
    "        else:\n",
    "            txt_paths['Other'].append(file_path)\n",
    "\n",
    "    return txt_paths, total_txt_files\n",
    "\n",
    "\n",
    "# Define supported classes\n",
    "CLASSES = [\n",
    "    'Healthy',\n",
    "    'Skew',\n",
    "    'Saccades',\n",
    "    'NystagmusLeft',\n",
    "    'NystagmusRight',\n",
    "    'NystagmusDownbeating',\n",
    "    'Other'\n",
    "]\n",
    "\n",
    "# Set data directory path\n",
    "DATA_DIRECTORY = '../RawData'\n",
    "\n",
    "# Extract file paths and get statistics\n",
    "txt_paths, total_txt_files = extract_latest_txt_files(DATA_DIRECTORY, CLASSES)\n",
    "\n",
    "# Print summary statistics\n",
    "print('Total txt files found:', total_txt_files)\n",
    "print(\n",
    "    'Total of unique newest txt files found: ',\n",
    "    len(sum(txt_paths.values(), [])),\n",
    "    txt_paths.values()\n",
    ")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f'{class_name}:', len(txt_paths[class_name]), txt_paths[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create an output folder 'csv_per_patient_per_measurement_action', that has:\n",
    "- Structure 'classes = ['Healthy', 'Skew', 'Saccades', 'NystagmusLeft', 'NystagmusRight', 'NystagmusDownbeating', 'Other']'\n",
    "- Folders per subject/patient inside classes folder.\n",
    "- .csv file per 'measurement_actions = ['beforeStart', 'startRecording', 'raum', 'still', 'nase', 'links', 'rechts', 'decke', 'bodem', 'blingHeadTest']'\n",
    "- .csv file has headers which are the different features 'timestamp, worldLeftEyeCoordinates, headPosition, ...'\n",
    "Use the paths of latest .txt file per patient 'txt_paths' from previous block.\n",
    "'''\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def convert_subject_data_to_csv(\n",
    "    category: str,\n",
    "    path_measurements: str,\n",
    "    output_base_path: str\n",
    ") -> None:\n",
    "    \"\"\"Convert raw subject data from txt to multiple CSV files.\n",
    "\n",
    "    Processes a single subject's raw data file and splits it into multiple CSV\n",
    "    files, one for each measurement action (e.g., raum, still, nase, etc.).\n",
    "    Handles coordinate data in various formats and saves them in a structured\n",
    "    directory hierarchy.\n",
    "\n",
    "    Args:\n",
    "        category: Subject category (e.g., 'Healthy', 'Skew', etc.)\n",
    "        path_measurements: Path to the raw data txt file\n",
    "        output_base_path: Base path where processed CSV files will be saved\n",
    "\n",
    "    Directory structure created:\n",
    "        output_base_path/\n",
    "            category/\n",
    "                patient_id/\n",
    "                    measurement_action.csv\n",
    "    \"\"\"\n",
    "    def _create_output_dirs(base_path: str, category: str,\n",
    "                          patient_id: str) -> str:\n",
    "        \"\"\"Create necessary output directories if they don't exist.\"\"\"\n",
    "        paths = [\n",
    "            base_path,\n",
    "            os.path.join(base_path, category),\n",
    "            os.path.join(base_path, category, patient_id)\n",
    "        ]\n",
    "        for path in paths:\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        return paths[-1]\n",
    "\n",
    "    def _parse_timestamp(timestamp_str: str) -> datetime | None:\n",
    "        \"\"\"Parse timestamp string to datetime object.\"\"\"\n",
    "        if any(char.isalpha() for char in timestamp_str):\n",
    "            return None\n",
    "        return datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    def _process_coordinate_data(matches: List[str]) -> List[Any]:\n",
    "        \"\"\"Process coordinate data from regex matches.\"\"\"\n",
    "        values = []\n",
    "        for i, match in enumerate(matches):\n",
    "            parts = match.split(', ')\n",
    "            # Last group is quaternion (4 values), others are coordinates (3 values)\n",
    "            group_size = 4 if i == len(matches) - 1 else 3\n",
    "            \n",
    "            for j in range(0, len(parts), group_size):\n",
    "                group = parts[j:j + group_size]\n",
    "                if 'NotAvailable' in group:\n",
    "                    values.append('NotAvailable')\n",
    "                else:\n",
    "                    values.append(tuple(float(part) for part in group))\n",
    "        return values\n",
    "\n",
    "    # Define measurement actions in order\n",
    "    MEASUREMENT_ACTIONS = [\n",
    "        'beforeStart',\n",
    "        'startRecording',\n",
    "        'raum',\n",
    "        'still',\n",
    "        'nase',\n",
    "        'links',\n",
    "        'rechts',\n",
    "        'decke',\n",
    "        'bodem',\n",
    "        'blingHeadTest'\n",
    "    ]\n",
    "\n",
    "    # Setup output directory structure\n",
    "    patient_id = os.path.basename(os.path.dirname(path_measurements))\n",
    "    output_dir = _create_output_dirs(output_base_path, category, patient_id)\n",
    "\n",
    "    # Read and process the raw data\n",
    "    df = pd.read_csv(path_measurements, delimiter=\"\\t\")\n",
    "    headers = df.columns[0].split(', ')\n",
    "\n",
    "    prepared_data = []\n",
    "    measurement_type_counter = 0\n",
    "\n",
    "    # Process each row in the dataframe\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        row_data = row.tolist()[0]  # Get first column which contains all data\n",
    "        timestamp_str = row_data[:23]\n",
    "        timestamp = _parse_timestamp(timestamp_str)\n",
    "\n",
    "        # Save current measurement and start new one if timestamp is None\n",
    "        if timestamp is None and prepared_data:\n",
    "            measurement_df = pd.DataFrame(prepared_data, columns=headers)\n",
    "            try:\n",
    "                output_file = os.path.join(\n",
    "                    output_dir,\n",
    "                    f'{MEASUREMENT_ACTIONS[measurement_type_counter]}.csv'\n",
    "                )\n",
    "            except IndexError:\n",
    "                output_file = os.path.join(\n",
    "                    output_dir,\n",
    "                    f'_extraSave_{measurement_type_counter}.csv'\n",
    "                )\n",
    "            measurement_df.to_csv(output_file, index=False)\n",
    "            prepared_data = []\n",
    "            measurement_type_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Process coordinate data\n",
    "        if timestamp is not None:\n",
    "            matches = re.findall(r'\\((.*?)\\)', row_data)\n",
    "            values = [timestamp] + _process_coordinate_data(matches)\n",
    "            prepared_data.append(values)\n",
    "\n",
    "\n",
    "def process_all_subjects(\n",
    "    txt_paths: dict,\n",
    "    output_base_path: str\n",
    ") -> None:\n",
    "    \"\"\"Process all subjects' data files.\n",
    "\n",
    "    Args:\n",
    "        txt_paths: Dictionary mapping categories to lists of file paths\n",
    "        output_base_path: Base path for output CSV files\n",
    "    \"\"\"\n",
    "    for class_name, paths in txt_paths.items():\n",
    "        print(f'Starting converting for group: {class_name}')\n",
    "        \n",
    "        for txt_path in paths:\n",
    "            convert_subject_data_to_csv(class_name, txt_path, output_base_path)\n",
    "        \n",
    "        output_category_path = os.path.join(output_base_path, class_name)\n",
    "        print(f'Finished converting for group: {class_name}')\n",
    "        print(f'Saved in: {output_category_path}\\n')\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    OUTPUT_PATH = Path(r'../data/csv_per_patient_per_measurement_action')\n",
    "    process_all_subjects(txt_paths, str(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process eye-tracking data and organize it into features and labels for ML.\n",
    "\n",
    "This module handles the conversion of raw CSV data into structured features and\n",
    "labels, organizing them by measurement action type and splitting coordinate data\n",
    "into separate components.\n",
    "\n",
    "measurement_actions = ['beforeStart', 'startRecording', 'raum', 'still', 'nase', 'links', 'rechts', 'decke', 'bodem', 'blingHeadTest']\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Process and organize eye-tracking data for machine learning.\"\"\"\n",
    "\n",
    "    # Class-level constants with explicit label mapping\n",
    "    CLASS_MAPPING = {\n",
    "        'Healthy': 0,\n",
    "        'Skew': 1,\n",
    "        'Saccades': 2,\n",
    "        'NystagmusLeft': 3,\n",
    "        'NystagmusRight': 4,\n",
    "        'NystagmusDownbeating': 5,\n",
    "        'Other': 6\n",
    "    }\n",
    "\n",
    "    MEASUREMENT_ACTIONS = [\n",
    "        'raum',\n",
    "        'still',\n",
    "        'nase',\n",
    "        'links',\n",
    "        'rechts',\n",
    "        'decke',\n",
    "        'bodem',\n",
    "        'blingHeadTest'\n",
    "    ]\n",
    "\n",
    "    XYZ_FEATURES = [\n",
    "        'worldLeftEyePosition',\n",
    "        'worldRightEyePosition',\n",
    "        'worldLeftEyeDirection',\n",
    "        'worldRightEyeDirection',\n",
    "        'worldCombinedEyePosition',\n",
    "        'worldCombinedEyeDirection',\n",
    "        'cameraLeftEyePosition',\n",
    "        'cameraRightEyePosition',\n",
    "        'cameraLeftEyeDirection',\n",
    "        'cameraRightEyeDirection',\n",
    "        'cameraCombinedEyePosition',\n",
    "        'cameraCombinedEyeDirection',\n",
    "        'headPosition',\n",
    "        'headEulerAngles'\n",
    "    ]\n",
    "\n",
    "    QUATERNION_FEATURES = ['headQuaternion ']\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dir: str,\n",
    "        output_dir: str\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the data processor.\n",
    "\n",
    "        Args:\n",
    "            input_dir: Path to directory containing input CSV files\n",
    "            output_dir: Path to directory where processed files will be saved\n",
    "        \"\"\"\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.measurement_counter = {\n",
    "            action: 0 for action in self.MEASUREMENT_ACTIONS\n",
    "        }\n",
    "        \n",
    "        # Verify input directory structure\n",
    "        self._verify_input_structure()\n",
    "\n",
    "    def _verify_input_structure(self) -> None:\n",
    "        \"\"\"Verify that the input directory structure matches expected classes.\"\"\"\n",
    "        if not self.input_dir.exists():\n",
    "            raise ValueError(f\"Input directory does not exist: {self.input_dir}\")\n",
    "\n",
    "        found_classes = {\n",
    "            path.name for path in self.input_dir.iterdir() \n",
    "            if path.is_dir()\n",
    "        }\n",
    "        expected_classes = set(self.CLASS_MAPPING.keys())\n",
    "        \n",
    "        missing_classes = expected_classes - found_classes\n",
    "        unexpected_classes = found_classes - expected_classes\n",
    "        \n",
    "        if missing_classes:\n",
    "            raise ValueError(\n",
    "                f\"Missing expected class directories: {missing_classes}\"\n",
    "            )\n",
    "        if unexpected_classes:\n",
    "            raise ValueError(\n",
    "                f\"Found unexpected class directories: {unexpected_classes}\"\n",
    "            )\n",
    "\n",
    "    def setup_directory_structure(self) -> None:\n",
    "        \"\"\"Create the output directory structure for features and labels.\"\"\"\n",
    "        for action in self.MEASUREMENT_ACTIONS:\n",
    "            action_path = self.output_dir / action\n",
    "            features_path = action_path / 'features'\n",
    "            labels_path = action_path / 'labels'\n",
    "\n",
    "            for path in [action_path, features_path, labels_path]:\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def process_coordinates(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature: str,\n",
    "        components: int\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process coordinate data from string format to separate columns.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame containing the feature\n",
    "            feature: Name of the feature to process\n",
    "            components: Number of components (3 for xyz, 4 for quaternions)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with processed coordinate columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Split the string and convert to numeric values\n",
    "            split_df = df[feature].str.strip('()').str.split(', ', expand=True)\n",
    "            split_df = split_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Handle missing columns\n",
    "            if split_df.shape[1] < components:\n",
    "                for i in range(components - split_df.shape[1]):\n",
    "                    split_df[split_df.shape[1] + i] = np.nan\n",
    "\n",
    "            # Assign new columns and drop original\n",
    "            suffixes = ['w', 'x', 'y', 'z'][:components] if components == 4 else ['x', 'y', 'z']\n",
    "            for i, suffix in enumerate(suffixes):\n",
    "                df[f'{feature}_{suffix}'] = split_df[i]\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {feature}: {str(e)}\")\n",
    "            # Create NaN columns if processing fails\n",
    "            suffixes = ['w', 'x', 'y', 'z'][:components] if components == 4 else ['x', 'y', 'z']\n",
    "            for suffix in suffixes:\n",
    "                df[f'{feature}_{suffix}'] = np.nan\n",
    "            if feature in df.columns:\n",
    "                df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process the DataFrame by converting timestamps and coordinates.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to process\n",
    "\n",
    "        Returns:\n",
    "            Processed DataFrame with converted timestamps and split coordinates\n",
    "        \"\"\"\n",
    "        # Convert timestamp and calculate time differences\n",
    "        df['timestamp'] = pd.to_datetime(\n",
    "            df['timestamp'],\n",
    "            format='%Y-%m-%d %H:%M:%S.%f'\n",
    "        )\n",
    "        df['delta_time'] = df['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "        # Reorder columns\n",
    "        df = df.reindex(columns=['timestamp', 'delta_time'] + [\n",
    "            col for col in df.columns\n",
    "            if col not in ['timestamp', 'delta_time']\n",
    "        ])\n",
    "\n",
    "        # Process xyz and quaternion features\n",
    "        for feature in self.XYZ_FEATURES:\n",
    "            if feature in df.columns:\n",
    "                df = self.process_coordinates(df, feature, 3)\n",
    "        for feature in self.QUATERNION_FEATURES:\n",
    "            if feature in df.columns:\n",
    "                df = self.process_coordinates(df, feature, 4)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_files(self) -> None:\n",
    "        \"\"\"Process all CSV files and organize them into features and labels.\"\"\"\n",
    "        self.setup_directory_structure()\n",
    "        print(\"Starting data processing...\")\n",
    "\n",
    "        # Track processed files for verification\n",
    "        processed_files = {action: [] for action in self.MEASUREMENT_ACTIONS}\n",
    "\n",
    "        # Walk through the input directory structure\n",
    "        for class_path in self.input_dir.iterdir():\n",
    "            if not class_path.is_dir() or class_path.name not in self.CLASS_MAPPING:\n",
    "                continue\n",
    "\n",
    "            class_name = class_path.name\n",
    "            label = self.CLASS_MAPPING[class_name]  # Use explicit mapping\n",
    "            print(f\"\\nProcessing class: {class_name} (Label: {label})\")\n",
    "\n",
    "            # Process each patient directory\n",
    "            for patient_path in class_path.iterdir():\n",
    "                if not patient_path.is_dir():\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing patient: {patient_path.name}\")\n",
    "\n",
    "                # Process each measurement file\n",
    "                for measurement in self.MEASUREMENT_ACTIONS:\n",
    "                    csv_path = patient_path / f\"{measurement}.csv\"\n",
    "                    if not csv_path.exists():\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # Read and process the file\n",
    "                        df = pd.read_csv(csv_path)\n",
    "                        df_processed = self.process_dataframe(df)\n",
    "\n",
    "                        # Save processed data\n",
    "                        counter = self.measurement_counter[measurement]\n",
    "                        features_path = (\n",
    "                            self.output_dir / measurement / 'features' / f'{counter}.csv'\n",
    "                        )\n",
    "                        labels_path = (\n",
    "                            self.output_dir / measurement / 'labels' / f'{counter}.csv'\n",
    "                        )\n",
    "\n",
    "                        df_processed.to_csv(features_path, index=False)\n",
    "                        labels_path.write_text(str(label))\n",
    "\n",
    "                        # Track processed file\n",
    "                        processed_files[measurement].append({\n",
    "                            'source_path': csv_path,\n",
    "                            'class': class_name,\n",
    "                            'label': label,\n",
    "                            'output_index': counter\n",
    "                        })\n",
    "\n",
    "                        self.measurement_counter[measurement] += 1\n",
    "                        print(f\"Processed: {measurement} (Label: {label})\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "\n",
    "        # Print verification summary\n",
    "        print(\"\\nProcessing verification:\")\n",
    "        for measurement, files in processed_files.items():\n",
    "            print(f\"\\n{measurement}:\")\n",
    "            for file in files:\n",
    "                print(f\"  {file['source_path'].parent.parent.name} \"\n",
    "                      f\"(Label: {file['label']}) -> {file['output_index']}.csv\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data processing pipeline.\"\"\"\n",
    "    # Define input and output directories\n",
    "    input_dir = r'../data/csv_per_patient_per_measurement_action'\n",
    "    output_dir = r'../data/hints_features_labels'\n",
    "\n",
    "    # Initialize and run processor\n",
    "    processor = DataProcessor(input_dir, output_dir)\n",
    "    processor.process_files()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(\"Files processed per measurement type:\")\n",
    "    for action, count in processor.measurement_counter.items():\n",
    "        print(f\"{action}: {count}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
